{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution of Website Design\n",
    "\n",
    "## Overview\n",
    "\n",
    "## This project analyzes the evolution of website design from the early 2000s, focusing on key aspects such as:\n",
    "\n",
    "- ### Number of images per year\n",
    "- ### Website size (in KB)\n",
    "- ### Number of ads on the website\n",
    "- ### Most used font\n",
    "- ### Average font size\n",
    "## Data was collected from arquivo.pt, a web archive, which provided snapshots of websites over time. The analysis covers various years (2000â€“2024), and the results are stored in a CSV file, then using data stored in CSV file visualized graphs were made to better understand the data.\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "- ### Python: For data scraping, processing, and analysis.\n",
    "- ### BeautifulSoup: For extracting website content and HTML structure.\n",
    "- ### Matplotlib: For visualizing the collected data in graphical form.\n",
    "- ### Pandas: For handling and processing CSV data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL's for data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    2000: \"https://arquivo.pt/noFrame/replay/20000606140117/http://www.sapo.pt/\",\n",
    "    2001: \"https://arquivo.pt/noFrame/replay/20010606140117/http://www.sapo.pt/\",\n",
    "    2002: \"https://arquivo.pt/noFrame/replay/20020606140117/http://www.sapo.pt/\",\n",
    "    2003: \"https://arquivo.pt/noFrame/replay/20030606140117/http://www.sapo.pt/\",\n",
    "    2004: \"https://arquivo.pt/noFrame/replay/20040606140117/http://www.sapo.pt/\",\n",
    "    2005: \"https://arquivo.pt/noFrame/replay/20050606140117/http://www.sapo.pt/\",\n",
    "    2006: \"https://arquivo.pt/noFrame/replay/20060606140117/http://www.sapo.pt/\",\n",
    "    2007: \"https://arquivo.pt/noFrame/replay/20070606140117/http://www.sapo.pt/\",\n",
    "    2008: \"https://arquivo.pt/noFrame/replay/20080606140117/http://www.sapo.pt/\",\n",
    "    2009: \"https://arquivo.pt/noFrame/replay/20090606140117/http://www.sapo.pt/\",\n",
    "    2010: \"https://arquivo.pt/noFrame/replay/20100202180215/http://www.sapo.pt/\",\n",
    "    2011: \"https://arquivo.pt/noFrame/replay/20110202180215/http://www.sapo.pt/\",\n",
    "    2012: \"https://arquivo.pt/noFrame/replay/20120202180215/http://www.sapo.pt/\",\n",
    "    2013: \"https://arquivo.pt/noFrame/replay/20130202180215/http://www.sapo.pt/\",\n",
    "    2014: \"https://arquivo.pt/noFrame/replay/20140202180215/http://www.sapo.pt/\",\n",
    "    2015: \"https://arquivo.pt/noFrame/replay/20150202180215/http://www.sapo.pt/\",\n",
    "    2016: \"https://arquivo.pt/noFrame/replay/20160202180215/http://www.sapo.pt/\",\n",
    "    2017: \"https://arquivo.pt/noFrame/replay/20170202180215/http://www.sapo.pt/\",\n",
    "    2018: \"https://arquivo.pt/noFrame/replay/20180202180215/http://www.sapo.pt/\",\n",
    "    2019: \"https://arquivo.pt/noFrame/replay/20190202180215/http://www.sapo.pt/\",\n",
    "    2020: \"https://arquivo.pt/noFrame/replay/20200202180215/http://www.sapo.pt/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzation of www.sapo.pt in 2000, 2010 and 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Photos and videos count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2000...\n",
      "Fetching data for 2001...\n",
      "Fetching data for 2002...\n",
      "Fetching data for 2003...\n",
      "Fetching data for 2004...\n",
      "Fetching data for 2005...\n",
      "Fetching data for 2006...\n",
      "Fetching data for 2007...\n",
      "Fetching data for 2008...\n",
      "Fetching data for 2009...\n",
      "Fetching data for 2010...\n",
      "Fetching data for 2011...\n",
      "Fetching data for 2012...\n",
      "Fetching data for 2013...\n",
      "Fetching data for 2014...\n",
      "Fetching data for 2015...\n",
      "Fetching data for 2016...\n",
      "Fetching data for 2017...\n",
      "Fetching data for 2018...\n",
      "Fetching data for 2019...\n",
      "Fetching data for 2020...\n",
      "Data saved to 'ImageCount.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def analyze_media(archived_url):\n",
    "    if archived_url:\n",
    "        html_content = requests.get(archived_url).text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        images = soup.find_all('img')\n",
    "        \n",
    "        return {\n",
    "            'images': len(images)\n",
    "        }\n",
    "    return {\n",
    "        'images': 0\n",
    "    }\n",
    "\n",
    "def collect_data(urls):\n",
    "    data = []\n",
    "    \n",
    "    for year, url in urls.items():\n",
    "        print(f\"Fetching data for {year}...\")\n",
    "        media_data = analyze_media(url)\n",
    "        data.append({'year': year, 'image_count': media_data['images']})\n",
    "\n",
    "    with open('ImageCount.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['year', 'image_count'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"Data saved to 'ImageCount.csv'\")\n",
    "\n",
    "collect_data(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Calculating website size in bytes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2000...\n",
      "Fetching data for 2001...\n",
      "Fetching data for 2002...\n",
      "Fetching data for 2003...\n",
      "Fetching data for 2004...\n",
      "Fetching data for 2005...\n",
      "Fetching data for 2006...\n",
      "Fetching data for 2007...\n",
      "Fetching data for 2008...\n",
      "Fetching data for 2009...\n",
      "Fetching data for 2010...\n",
      "Fetching data for 2011...\n",
      "Fetching data for 2012...\n",
      "Fetching data for 2013...\n",
      "Fetching data for 2014...\n",
      "Fetching data for 2015...\n",
      "Fetching data for 2016...\n",
      "Fetching data for 2017...\n",
      "Fetching data for 2018...\n",
      "Fetching data for 2019...\n",
      "Fetching data for 2020...\n",
      "Data saved to 'WebsiteSize.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_page_size(archived_url):\n",
    "    try:\n",
    "        response = requests.get(archived_url)\n",
    "        if response.ok:\n",
    "            page_size = len(response.content)\n",
    "            return page_size\n",
    "        else:\n",
    "            print(f\"Error fetching page: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def collect_data(urls):\n",
    "    data = []\n",
    "\n",
    "    for year, url in urls.items():\n",
    "        print(f\"Fetching data for {year}...\")\n",
    "        size = get_page_size(url)\n",
    "        if size is not None:\n",
    "            data.append({'year': year, 'size': size})\n",
    "        else:\n",
    "            print(f\"Year {year}: Failed to fetch size\")\n",
    "\n",
    "    with open('WebsiteSize.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['year', 'size'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"Data saved to 'WebsiteSize.csv'\")\n",
    "\n",
    "collect_data(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Number of ads:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2000...\n",
      "Fetching data for 2001...\n",
      "Fetching data for 2002...\n",
      "Fetching data for 2003...\n",
      "Fetching data for 2004...\n",
      "Fetching data for 2005...\n",
      "Fetching data for 2006...\n",
      "Fetching data for 2007...\n",
      "Fetching data for 2008...\n",
      "Fetching data for 2009...\n",
      "Fetching data for 2010...\n",
      "Fetching data for 2011...\n",
      "Fetching data for 2012...\n",
      "Fetching data for 2013...\n",
      "Fetching data for 2014...\n",
      "Fetching data for 2015...\n",
      "Fetching data for 2016...\n",
      "Fetching data for 2017...\n",
      "Fetching data for 2018...\n",
      "Fetching data for 2019...\n",
      "Fetching data for 2020...\n",
      "Data saved to 'NumberOfAdds.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def fetch_archived_page(archived_url):\n",
    "    try:\n",
    "        response = requests.get(archived_url)\n",
    "        if response.ok:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch page: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_ads(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    ad_divs = soup.find_all('div', class_=lambda x: x and ('ad' in x or 'advertisement' in x.lower()))\n",
    "    iframes = soup.find_all('iframe')\n",
    "    script_tags = soup.find_all('script')\n",
    "\n",
    "    ads_count = len(ad_divs) + len(iframes) + len(script_tags)\n",
    "\n",
    "    return {'ads_count': ads_count}\n",
    "\n",
    "def collect_data(urls):\n",
    "    data = []\n",
    "\n",
    "    for year, url in urls.items():\n",
    "        print(f\"Fetching data for {year}...\")\n",
    "        html_content = fetch_archived_page(url)\n",
    "        ads_data = analyze_ads(html_content)\n",
    "        data.append({'year': year, 'adds': ads_data['ads_count']})\n",
    "\n",
    "    with open('NumberOfAdds.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['year', 'adds'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"Data saved to 'NumberOfAdds.csv'\")\n",
    "\n",
    "collect_data(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Most used font and average font size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2000...\n",
      "2000: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2001...\n",
      "2001: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2002...\n",
      "2002: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2003...\n",
      "2003: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2004...\n",
      "2004: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2005...\n",
      "2005: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2006...\n",
      "2006: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2007...\n",
      "2007: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2008...\n",
      "2008: Average Font Size = 14.59px, Most Used Font = Arial, Helvetica, sans-serif (1 times)\n",
      "Fetching data for 2009...\n",
      "2009: Average Font Size = 14.43px, Most Used Font = Arial, Helvetica, sans-serif (1 times)\n",
      "Fetching data for 2010...\n",
      "2010: Average Font Size = 14.74px, Most Used Font = 'american typewriter',Georgia,serif (3 times)\n",
      "Fetching data for 2011...\n",
      "2011: Average Font Size = 14.99px, Most Used Font = 'american typewriter',Georgia,serif (3 times)\n",
      "Fetching data for 2012...\n",
      "2012: Average Font Size = 14.95px, Most Used Font = 'american typewriter',Georgia,serif (3 times)\n",
      "Fetching data for 2013...\n",
      "2013: Average Font Size = 14.52px, Most Used Font = 'Museo Sans 500','Segoe UI',sans-serif (3 times)\n",
      "Fetching data for 2014...\n",
      "2014: Average Font Size = 13.66px, Most Used Font = 'Segoe UI',Arial,sans-serif (7 times)\n",
      "Fetching data for 2015...\n",
      "2015: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2016...\n",
      "2016: Average Font Size = 0.00px, Most Used Font = 'Roboto' (2 times)\n",
      "Fetching data for 2017...\n",
      "2017: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2018...\n",
      "2018: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2019...\n",
      "2019: Average Font Size = 0.00px, Most Used Font = None (0 times)\n",
      "Fetching data for 2020...\n",
      "2020: Average Font Size = 12.57px, Most Used Font = None (0 times)\n",
      "\n",
      "Typography analysis data saved to 'TypographyReport.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def fetch_archived_page(archived_url):\n",
    "    try:\n",
    "        response = requests.get(archived_url)\n",
    "        return response.text if response.ok else None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_css_file(css_url):\n",
    "    try:\n",
    "        response = requests.get(css_url)\n",
    "        return response.text if response.ok else None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_typography(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'span'])\n",
    "    font_sizes, font_families = [], []\n",
    "\n",
    "    for element in text_elements:\n",
    "        style = element.get('style')\n",
    "        if style:\n",
    "            if 'font-size' in style:\n",
    "                font_sizes.append(style.split('font-size:')[-1].split(';')[0].strip())\n",
    "            if 'font-family' in style:\n",
    "                font_families.append(style.split('font-family:')[-1].split(';')[0].strip())\n",
    "\n",
    "    external_links = soup.find_all('link', rel=\"stylesheet\")\n",
    "    for link in external_links:\n",
    "        css_href = link.get('href')\n",
    "        if css_href:\n",
    "            css_url = css_href if css_href.startswith('http') else f\"https://arquivo.pt{css_href}\"\n",
    "            css_content = fetch_css_file(css_url)\n",
    "            if css_content:\n",
    "                extract_fonts_from_css(css_content, font_sizes, font_families)\n",
    "\n",
    "    return font_sizes, font_families\n",
    "\n",
    "def extract_fonts_from_css(css_content, font_sizes, font_families):\n",
    "    font_sizes.extend(re.findall(r'font-size:\\s*([^;]+);', css_content))\n",
    "    font_families.extend(re.findall(r'font-family:\\s*([^;]+);', css_content))\n",
    "\n",
    "def calculate_average_font_size(font_sizes):\n",
    "    total_size, count = 0, 0\n",
    "    for size in font_sizes:\n",
    "        size_value = re.findall(r'(\\d*\\.?\\d+)', size)\n",
    "        if size_value:\n",
    "            value = float(size_value[0])\n",
    "            if 'px' in size:\n",
    "                total_size += value\n",
    "            elif 'em' in size:\n",
    "                total_size += value * 16\n",
    "            elif '%' in size:\n",
    "                total_size += (value / 100) * 16\n",
    "            count += 1\n",
    "    return total_size / count if count > 0 else 0\n",
    "\n",
    "def get_most_used_font(font_families):\n",
    "    if font_families:\n",
    "        font_count = Counter(font_families)\n",
    "        most_common_font = font_count.most_common(1)[0]\n",
    "        return most_common_font[0], most_common_font[1]\n",
    "    return None, 0\n",
    "\n",
    "def collect_data_to_csv(archived_urls):\n",
    "    data = []\n",
    "\n",
    "    for year, url in archived_urls.items():\n",
    "        print(f\"Fetching data for {year}...\")\n",
    "        html_content = fetch_archived_page(url)\n",
    "        if html_content:\n",
    "            font_sizes, font_families = analyze_typography(html_content)\n",
    "            average_font_size = calculate_average_font_size(font_sizes)\n",
    "            most_used_font, count = get_most_used_font(font_families)\n",
    "            data.append({\n",
    "                'year': year,\n",
    "                'average_font_size_px': average_font_size,\n",
    "                'most_used_font': most_used_font,\n",
    "                'font_usage_count': count\n",
    "            })\n",
    "            print(f\"{year}: Average Font Size = {average_font_size:.2f}px, Most Used Font = {most_used_font} ({count} times)\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page for the year {year}.\")\n",
    "            data.append({\n",
    "                'year': year,\n",
    "                'average_font_size_px': None,\n",
    "                'most_used_font': None,\n",
    "                'font_usage_count': 0\n",
    "            })\n",
    "\n",
    "    # Save to CSV\n",
    "    with open('TypographyReport.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['year', 'average_font_size_px', 'most_used_font', 'font_usage_count'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"\\nTypography analysis data saved to 'TypographyReport.csv'\")\n",
    "\n",
    "collect_data_to_csv(urls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
